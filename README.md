# ML Project

End-to-end Iris classifier built with scikit-learn. The project automates data ingestion, transformation, training, and artifact storage behind a simple pipeline.

## ğŸ§° Tech Stack

- Python 3.8 (recommended via Conda)
- pandas, scikit-learn, numpy
- Logging and custom exception handling

## ğŸš€ Quick Start

1. **Clone & enter the project directory**
   ```bash
   git clone <repo-url>
   cd mlproject
   ```

2. **Create a Conda environment (Python 3.8)**
   ```bash
   conda create -n iris-env python=3.8 -y
   conda activate iris-env
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Verify the dataset**
   - Ensure `data/Iris.csv` exists (same schema as the classic Iris dataset).
   - The ingestion pipeline copies it into `artifacts/`.

5. **Run the training pipeline**
   ```bash
   python -m src.pipeline.train_pipeline
   ```
   This performs:
   - Data ingestion â†’ saves `artifacts/data.csv`, `artifacts/train.csv`, `artifacts/test.csv`
   - Data transformation â†’ standard scaling + serialized `artifacts/scaler.pkl`
   - Model training (Logistic Regression) â†’ saves `artifacts/model.pkl`
   - Logs accuracy in the console and `src/logs/`.

6. **(Optional) Explore via notebook**
   - Open `ml_notebook.ipynb` for an interactive walk-through: ingestion preview, transformation, training, and sample predictions.

7. **Use the trained model for prediction**
   ```python
   from src.components.model_trainer import ModelTrainer
   import pandas as pd

   # load artifacts if needed
   trainer = ModelTrainer()
   scaler = pickle.load(open("artifacts/scaler.pkl", "rb"))
   model = pickle.load(open("artifacts/model.pkl", "rb"))

   sample = pd.read_csv("artifacts/test.csv").drop(columns=["Species"]).head()
   sample_scaled = scaler.transform(sample)
   preds = model.predict(sample_scaled)
   print(preds)
   ```

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ data/                # raw source data (Iris.csv)
â”œâ”€â”€ artifacts/           # outputs: data splits, scaler, trained model
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/      # ingestion, transformation, training modules
â”‚   â”œâ”€â”€ pipeline/        # orchestration scripts
â”‚   â”œâ”€â”€ utils.py         # common helpers (save/load objects, etc.)
â”‚   â”œâ”€â”€ logger.py        # central logging config
â”‚   â””â”€â”€ exception.py     # custom exception wrapper
â”œâ”€â”€ ml_notebook.ipynb    # exploration & demo notebook
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ› ï¸ Troubleshooting

- **Missing `Species` column**: check CSV headers; they are case-insensitive but must exist.
- **Model attribute missing**: ensure you call `initiate_model_training` before trying to access `trainer.model`.
- **Artifacts not updating**: delete the `artifacts/` folder and rerun the pipeline to regenerate fresh outputs.

Happy experimenting! ğŸ‘©â€ğŸ”¬ğŸ‘¨â€ğŸ”¬